---
summary: AI50 RAG Pipeline Codelab
id: ai50-rag-pipeline
---
# Project ORBIT â€” PE Dashboard Factory for Forbes AI 50

**Automated Private-Equity Intelligence System**

## ğŸ“‹ Project Overview

Project ORBIT is an automated, reproducible, cloud-hosted system that generates investor dashboards for all 50 Forbes AI 50 companies. The system uses two parallel generation pipelines (RAG and Structured) to extract and analyze company data, then serves dashboards through FastAPI and Streamlit on Google Cloud Platform.

## ğŸ“˜ Interactive Codelab  
View the full tutorial here:  
ğŸ‘‰ [Open Google Codelab](https://codelabs-preview.appspot.com/?file_id=https://raw.githubusercontent.com/Effyrt/AI50-rag-pipeline/refs/heads/main/codelabs.md)

### Key Features

- **Dual Pipeline Architecture**: 
  - **Structured Pipeline**: Uses Pydantic + Instructor for precise data extraction
  - **RAG Pipeline**: Uses vector database (FAISS) for retrieval-augmented generation
  
- **Automated Data Ingestion**: 
  - Web scraping with Playwright (homepage, about, products, careers, blog, etc.)
  - Footer link detection for comprehensive page discovery
  - Daily automated refresh via Apache Airflow

- **5-Pass Structured Extraction**:
  - Pass 1: Company basics + Events (funding, partnerships, milestones)
  - Pass 2: Products + Leadership
  - Pass 3: GitHub visibility metrics
  - Pass 4: Business Intelligence (value prop, competitors, GTM)
  - Pass 5: Employee & Hiring data

- **Cloud-Native Deployment**:
  - Google Cloud Platform (GCP) infrastructure
  - Cloud Run Jobs for scalable execution
  - Cloud Composer (Airflow) for orchestration
  - Google Cloud Storage (GCS) for data persistence

- **Dashboard Generation**:
  - 8-section investor dashboards (Company Overview, Business Model, Funding, Growth, Visibility, Risks, Outlook, Disclosure Gaps)
  - FastAPI endpoints for both RAG and Structured pipelines
  - Streamlit UI for interactive dashboard viewing

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Google Cloud Platform                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Cloud Run      â”‚       â”‚   Cloud Run      â”‚           â”‚
â”‚  â”‚  (FastAPI)       â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚  (Streamlit)     â”‚           â”‚
â”‚  â”‚  Port: 8000      â”‚       â”‚  Port: 8501      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚                         â”‚                      â”‚
â”‚           â”‚                         â”‚                      â”‚
â”‚           â–¼                         â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚        Google Cloud Storage (GCS)               â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚       â”‚
â”‚  â”‚  â”‚   raw/   â”‚  â”‚structured/â”‚  â”‚payloads/ â”‚      â”‚       â”‚
â”‚  â”‚  â”‚          â”‚  â”‚           â”‚  â”‚          â”‚      â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚       â”‚
â”‚  â”‚  â”‚vector_   â”‚                                   â”‚       â”‚
â”‚  â”‚  â”‚index/    â”‚                                   â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â–²                                                â”‚
â”‚           â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”                                     â”‚
â”‚  â”‚ Cloud Composer    â”‚                                     â”‚
â”‚  â”‚  (Airflow)        â”‚                                     â”‚
â”‚  â”‚                   â”‚                                     â”‚
â”‚  â”‚  - Scraper Job    â”‚                                     â”‚
â”‚  â”‚  - Extractor Job  â”‚                                     â”‚
â”‚  â”‚  - RAG Index Job  â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Google Cloud Platform account with billing enabled
- `gcloud` CLI installed and configured
- OpenAI API key (for LLM extraction)

### Local Setup

```bash
# Clone repository
git clone https://github.com/Effyrt/AI50-rag-pipeline.git
cd AI50-rag-pipeline

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env  # Create .env file
# Add your OPENAI_API_KEY to .env
```

### Run Locally (Development)

```bash
# Start FastAPI backend
uvicorn src.api:app --reload

# In another terminal, start Streamlit frontend
streamlit run src/streamlit_app.py
```

- FastAPI: http://localhost:8000
- Streamlit: http://localhost:8501

### Docker (Local Testing)

```bash
cd docker
docker compose up --build
```

## â˜ï¸ GCP Deployment

### Initial Setup

1. **Set up GCP infrastructure:**
   ```bash
   cd gcp
   ./setup_gcp.sh
   ```
   This creates:
   - GCS buckets (raw-data, structured-data, payloads, vector-index)
   - Secret Manager for API keys
   - Service accounts with proper IAM roles

2. **Build and deploy Docker images:**
   ```bash
   ./build_and_deploy.sh
   ```
   This builds and deploys:
   - `ai50-scraper` Cloud Run Job
   - `ai50-extractor` Cloud Run Job
   - `ai50-rag-index-builder` Cloud Run Job (if RAG pipeline is deployed)

3. **Set up Cloud Composer (Airflow):**
   ```bash
   ./setup_composer.sh
   ```
   This creates:
   - Cloud Composer environment
   - Uploads DAGs to Composer
   - Configures service accounts

### Running the Pipeline

1. **Manual trigger via Airflow UI:**
   - Access Airflow UI (link provided after Composer setup)
   - Trigger `ai50_daily_refresh` DAG
   - Monitor execution in Airflow UI

2. **Automatic daily refresh:**
   - DAG runs daily at 3 AM UTC
   - Scrapes updated pages
   - Extracts structured data
   - Updates dashboards

### Documentation

- **GCP Deployment Guide**: See `docs/GCP_DEPLOYMENT_GUIDE.md`
- **Airflow Usage Guide**: See `docs/AIRFLOW_USAGE_GUIDE.md`
- **RAG Pipeline Guide**: See `TEAMMATE_RAG_GUIDE.md`

## ğŸ“Š Data Flow

1. **Ingestion**: Airflow DAG â†’ Scraper Job â†’ Raw HTML/text â†’ GCS `raw-data/`
2. **Structured Extraction**: Airflow DAG â†’ Extractor Job â†’ Structured JSON â†’ GCS `structured-data/`
3. **RAG Index Building**: Airflow DAG â†’ RAG Index Job â†’ Vector Index â†’ GCS `vector-index/`
4. **Payload Assembly**: Structured data â†’ Combined payloads â†’ GCS `payloads/`
5. **Dashboard Generation**: 
   - RAG: Vector retrieval â†’ LLM â†’ Dashboard markdown
   - Structured: Payload â†’ LLM â†’ Dashboard markdown
6. **Serving**: FastAPI + Streamlit â†’ Display dashboards

## ğŸ“ Project Structure

```
AI50-rag-pipeline/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ playwright_scraper.py      # Web scraper (Playwright)
â”‚   â”œâ”€â”€ scraper_gcp.py            # GCP scraper entry point
â”‚   â”œâ”€â”€ extractor_v4_bi.py        # 5-pass structured extractor
â”‚   â”œâ”€â”€ extractor_gcp.py          # GCP extractor entry point
â”‚   â”œâ”€â”€ rag_pipeline.py            # RAG pipeline (teammate)
â”‚   â”œâ”€â”€ models.py                  # Pydantic data models
â”‚   â”œâ”€â”€ api.py                    # FastAPI endpoints
â”‚   â””â”€â”€ streamlit_app.py          # Streamlit UI
â”œâ”€â”€ airflow/dags/           # Airflow DAGs
â”‚   â””â”€â”€ ai50_daily_refresh.py
â”œâ”€â”€ gcp/                    # GCP deployment scripts
â”‚   â”œâ”€â”€ setup_gcp.sh
â”‚   â”œâ”€â”€ build_and_deploy.sh
â”‚   â””â”€â”€ setup_composer.sh
â”œâ”€â”€ docker/                 # Docker configurations
â”œâ”€â”€ docs/                   # Documentation
â””â”€â”€ data/                   # Data files (seed only)
    â””â”€â”€ forbes_ai50_seed.json
```

## ğŸ§ª Testing

### Test Scraping
```bash
python -m src.scraper_gcp
```

### Test Extraction
```bash
python -m src.extractor_gcp
```

### Test RAG Pipeline
```bash
python -m src.rag_pipeline
```

## ğŸ“ Key Technologies

- **Web Scraping**: Playwright, BeautifulSoup4
- **LLM & Extraction**: OpenAI GPT-4o-mini, Instructor, Pydantic
- **Vector DB**: FAISS
- **Orchestration**: Apache Airflow (Cloud Composer)
- **Cloud Platform**: Google Cloud Platform (Cloud Run, GCS, Cloud Composer)
- **API**: FastAPI
- **UI**: Streamlit
- **Containerization**: Docker

## ğŸ“š Assignment Details

**Course**: DAMG7245 â€” Assignment 2  
**Project**: Case Study 2 â€” Project ORBIT (Part 1)  
**Institution**: Northeastern University

## ğŸ‘¥ Team Contributions

### Contribution Attestation

**WE ATTEST THAT WE HAVEN'T USED ANY OTHER STUDENTS' WORK IN OUR ASSIGNMENT AND ABIDE BY THE POLICIES LISTED IN THE STUDENT HANDBOOK**

| Team Member | Contribution | Percentage |
|------------ |--------------|------------|
| **Hemanth Rayudu** | Structured Pipeline, Airflow DAGs, GCP Deployment | 33.33% |
| **PeiYing Chen** | RAG Pipeline Implementation | 33.33% |
| **Om Shailesh Raut** | Frontend & Backend (FastAPI + Streamlit) | 33.33% |

## ğŸ”— Links

- **GitHub Repository**: https://github.com/Effyrt/AI50-rag-pipeline
- **Project Video Demo**: https://drive.google.com/file/d/188NkhlREF0QHgGaySn_ZsDOnG95bkoVa/view?usp=sharing
- **GCP Project**: gen-lang-client-0653324487

## ğŸ“„ License

See `LICENSE` file for details.

## ğŸ™ Acknowledgments

- Forbes AI 50 for the company list
- OpenAI for GPT-4o-mini API
- Google Cloud Platform for infrastructure
- Apache Airflow for orchestration

---

**Last Updated**: November 2025  
**Status**: Production Ready âœ…
