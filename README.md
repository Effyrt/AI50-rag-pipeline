# AI50 RAG Pipeline - Integration Guide

**DAMG 7245 Assignment 4 - Forbes AI 50 PE Dashboard**

---

## ğŸ“‹ Overview

This repository contains the **RAG (Unstructured) Pipeline** for generating Private Equity investment dashboards. The system includes:
- Web scraping for 50 Forbes AI 50 companies
- Vector database with RAG retrieval
- Airflow orchestration for data ingestion
- FastAPI backend for dashboard generation

---

## âœ… Completed: RAG Unstructured Pipeline

### **Components Implemented**

| Component | File | Status | Description |
|-----------|------|--------|-------------|
| Web Scraper | `src/scraper.py` | âœ… | Scrapes 50 companies, 5 pages each |
| RAG Pipeline | `src/rag_pipeline.py` | âœ… | Vector DB + LLM generation |
| Batch Indexer | `src/index_all_companies.py` | âœ… | Batch index all companies to vector DB |
| FastAPI Backend | `src/api.py` | âœ… | `/dashboard/rag` endpoint |
| Full Ingest DAG | `dags/ai50_full_ingest_dag.py` | âœ… | One-time full scrape + index |
| Daily Refresh DAG | `dags/ai50_daily_refresh_dag.py` | âœ… | Daily incremental updates |


## ğŸ”„ RAG Pipeline Workflow

**What it does:**
1. Scans all company folders in `data/raw/`
2. Processes `.txt` files for each company
3. Creates document chunks and indexes to ChromaDB
4. Generates `data/indexing_results.json` with summary


## ğŸ”— How to Integrate Structured Pipeline with Airflow

---

## ğŸ”§ Integration Points for Structured Pipeline

### **Step 1: Add Structured Extraction Task**

In `dags/ai50_full_ingest_dag.py`, add a new task:

# Add to DAG
# Update task dependencies
t1_load >> t2_scrape >> t3_index >> t4_extract >> t5_report

---

### **Step 2: Daily Refresh Integration**

In `dags/ai50_daily_refresh_dag.py`, add task:

# Add to DAG
# Update dependencies
t1_load >> t2_refresh >> [t3_update_db, t4_update_payloads] >> t5_log

### **Step 3: FastAPI Backend Integration**
In `src/api.py` - have to combine with Structured workflow

---

## ğŸ“ Expected Project Structure After Integration
```
AI50-rag-pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ ai50_full_ingest_dag.py      # âœ… Includes RAG + Structured extraction
â”‚   â””â”€â”€ ai50_daily_refresh_dag.py    # âœ… Updates both pipelines
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py                        # âœ… Both /dashboard/rag and /dashboard/structured
â”‚   â”œâ”€â”€ rag_pipeline.py               # âœ… Complete
â”‚   â”œâ”€â”€ index_all_companies.py        # âœ… Batch indexer for vector DB
â”‚   â”œâ”€â”€ scraper.py                    # âœ… Complete
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ forbes_ai50_seed.json         # âœ… Input: 50 companies
â”‚   â”œâ”€â”€ indexing_results.json         # âœ… Generated by batch indexer
â”‚   â”œâ”€â”€ raw/                          # âœ… Output
â”‚   â”‚   â”œâ”€â”€ anthropic/
â”‚   â”‚   â”‚   â”œâ”€â”€ about.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ blog.html
â”‚   â”‚   â””â”€â”€ ... (50 files)
â”‚   â””â”€â”€ vector_db/                    # âœ… Store to Chroma DB. Generated by RAG pipeline
â”‚
â””â”€â”€ requirements.txt                  # âœ… Add: instructor>=0.4.0
```

---

## ğŸš€ Testing the Integration

### **Test Batch Indexing (RAG)**
```bash
# Index all companies to vector DB
python -m src.index_all_companies

# Check results
cat data/indexing_results.json
```

---

## ğŸ“¡ API Integration

### **Current RAG Endpoint**
```http
POST /dashboard/rag

Request:
{
  "company_name": "Anthropic",
  "top_k": 20
}

Response:
{
  "company_name": "Anthropic",
  "pipeline": "RAG (Unstructured)",
  "dashboard": "# Anthropic Dashboard\n...",
  "generated_at": "2025-11-07T..."
}
```

### **Combinition**
```python
# In src/api.py - have to combine with Structured workflow:

```

---

## ğŸ“¦ Dependencies

### **Already Installed (RAG Pipeline)**
```txt
fastapi==0.104.1
uvicorn==0.24.0
apache-airflow==2.7.3
langchain==0.1.0
chromadb==0.4.18
sentence-transformers==2.2.2
openai==1.3.0
pydantic==2.5.0
beautifulsoup4==4.12.2
requests==2.31.0
```

---

## ğŸ” Key Files for Integration

### **For Understanding RAG Pipeline:**

**`src/rag_pipeline.py`**
- `RAGPipeline.index_company(company_name, data_folder)` - How RAG indexes data
- `RAGPipeline.generate_dashboard(company_name, top_k)` - How RAG generates dashboards

**`src/index_all_companies.py`**
- **Batch indexing script** for processing all companies at once
- Scans `data/raw/` and indexes each company folder
- Generates `data/indexing_results.json` with success/failure summary
- **Use this for initial setup or manual re-indexing**

## ğŸ“Š Dashboard Format (Both Pipelines)

Both RAG and Structured pipelines MUST generate dashboards with these 8 sections:

1. **Company Overview**
2. **Business Model and GTM**
3. **Funding & Investor Profile**
4. **Growth Momentum**
5. **Visibility & Market Sentiment**
6. **Risks and Challenges**
7. **Outlook**
8. **Disclosure Gaps**

**Critical:** Use "Not disclosed." for missing information. Never invent data.
